{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiobjective_opt.neural_net.models.pytorch_cifar_models as models\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cifar100_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_params(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "cifar100_models = {}\n",
    "for model_class in dir(models):\n",
    "    if not \"100\" in model_class:\n",
    "        continue\n",
    "    model = getattr(models, model_class)()\n",
    "    if get_n_params(model) > 5e6:\n",
    "        continue\n",
    "    cifar100_models[model_class] = model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['cifar100_mobilenetv2_x0_5', 'cifar100_mobilenetv2_x0_75', 'cifar100_mobilenetv2_x1_0', 'cifar100_mobilenetv2_x1_4', 'cifar100_resnet20', 'cifar100_resnet32', 'cifar100_resnet44', 'cifar100_resnet56', 'cifar100_shufflenetv2_x0_5', 'cifar100_shufflenetv2_x1_0', 'cifar100_shufflenetv2_x1_5'])\n"
     ]
    }
   ],
   "source": [
    "print(cifar100_models.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment with computer vision model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "11 3 100 2 33\n",
      "2 9 18\n",
      "4 3 12\n",
      "11 1 11\n",
      "4 5 20\n",
      "11 1 11\n",
      "11 3 33\n",
      "total_steps: 105\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from experiment_code.neural.train.hyperband import HyperbandRunner, ModelSampler\n",
    "\n",
    "\n",
    "model_sampler = ModelSampler\n",
    "\n",
    "agent_runner = HyperbandRunner(model_sampler, \n",
    "                                max_iter= None,\n",
    "                                eta = 3,\n",
    "                                max_budget=100)\n",
    "agent_runner.estimate_steps(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.floor( 10 / 3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from dataclasses import MISSING, asdict, dataclass, field, fields\n",
    "from typing import List\n",
    "\n",
    "from multiobjective_opt.neural_net.models.cifar_parametrized import ResNet\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ResNetParams:\n",
    "    num_blocks: List = field(\n",
    "        default_factory=lambda: [2, 2, 2, 2]\n",
    "    )  # Количество блоков в каждом слое\n",
    "    num_filters: int = 32  # Количество фильтров в начальном слое\n",
    "    use_batchnorm: bool = False  # Использовать BatchNorm\n",
    "    use_dropout: bool = False  # Использовать Dropout\n",
    "    dropout_prob: float = 0.5  # Вероятность дропаут"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiment_code.neural.train_cifar import get_models\n",
    "models = [list[get_models().items()] for _ in range(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def max_iter_find(B, eta):\n",
    "    f = lambda x: np.log(eta * x) * x\n",
    "    b_eta = B * np.log(eta)\n",
    "    left = 1\n",
    "    right = 2\n",
    "    while f(right) < b_eta:\n",
    "        right *= 2\n",
    "\n",
    "    while abs(left - right) > 0.1:\n",
    "        mid = (left + right)/2\n",
    "        f_mid = f(mid)\n",
    "        if f_mid < b_eta:\n",
    "            left = mid\n",
    "        else:\n",
    "            right = mid\n",
    "    return np.ceil((left + right)/2)\n",
    "\n",
    "\n",
    "B = 100\n",
    "eta = 3\n",
    "\n",
    "print(max_iter_find(B, eta))\n",
    "\n",
    "m_it = max_iter_find(B, eta)\n",
    "(np.log(m_it)/np.log(eta) + 1) * m_it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ResNetParams()\n",
    "model = ResNet(**asdict(a))\n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
    "print(pytorch_total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "# Пример данных: список наблюдений (x_i, t_i)\n",
    "observations = [\n",
    "    (np.array([1, 2, 3]), np.array([0, 1, 2])),  # Наблюдение 1\n",
    "    (np.array([2, 3, 4]), np.array([0.5, 1.5, 2.5])),  # Наблюдение 2\n",
    "    (np.array([1.5, 2.5, 3.5]), np.array([0.2, 1.2, 2.2]))  # Наблюдение 3\n",
    "]\n",
    "\n",
    "def interpolated_graphs(values, times):\n",
    "    \"\"\"\n",
    "    values [n_runs x n_arms x n_observations]\n",
    "    times [n_runs x n_arms x n_observations]\n",
    "    \"\"\"\n",
    "# Шаг 1: Создать общую временную сетку\n",
    "min_time = min(min(t) for _, t in observations)\n",
    "max_time = max(max(t) for _, t in observations)\n",
    "time_grid = np.linspace(min_time, max_time, num=100)  # 100 точек на сетке\n",
    "\n",
    "# Шаг 2: Интерполировать данные на общей сетке\n",
    "interpolated_data = []\n",
    "for x, t in observations:\n",
    "    interp_func = interp1d(t, x, kind='linear', fill_value=\"extrapolate\")\n",
    "    interpolated_data.append(interp_func(time_grid))\n",
    "\n",
    "# Шаг 3: Агрегировать данные\n",
    "interpolated_data = np.array(interpolated_data)  # Преобразовать в массив NumPy\n",
    "\n",
    "# Среднее значение\n",
    "mean_values = np.mean(interpolated_data, axis=0)\n",
    "\n",
    "# Квантили (например, 0.1 и 0.9)\n",
    "quantile_10 = np.quantile(interpolated_data, q=0.1, axis=0)\n",
    "quantile_90 = np.quantile(interpolated_data, q=0.9, axis=0)\n",
    "\n",
    "# Результаты\n",
    "results = pd.DataFrame({\n",
    "    \"time\": time_grid,\n",
    "    \"mean\": mean_values,\n",
    "    \"quantile_10\": quantile_10,\n",
    "    \"quantile_90\": quantile_90\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from tabulate import tabulate\n",
    "with open(\"../exp_results/cv/cifar10_different.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "    data =data[:-1]\n",
    "\n",
    "headers = [\"Model name\", \"Train loss\", \"Model Accuracy\", \"Model loss\", \"runtime\", \"epochs\"]\n",
    "print(tabulate(data, headers=headers, floatfmt=\".3f\", tablefmt=\"heavy_outline\"))\n",
    "# print(tabulate(data, headers=headers, floatfmt=\".3f\", tablefmt=\"latex\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"../exp_results/cv/cifar10_different.json\", \"r\") as f:\n",
    "    bounds = json.load(f)\n",
    "\n",
    "def get_times():\n",
    "    model_time = {i: 0. for i in range(9)}\n",
    "    model_time[0] = bounds[0][1]\n",
    "    for i in range(1, len(bounds)):\n",
    "        model_time[bounds[i][0]] += bounds[i][1] - bounds[i-1][1]\n",
    "    return model_time\n",
    "\n",
    "times_online = get_times()\n",
    "\n",
    "import pickle\n",
    "from tabulate import tabulate\n",
    "with open(\"../exp_results/cv/cifar10_different.pkl\", \"rb\") as f:\n",
    "    data_mab = pickle.load(f)\n",
    "runtime = data_mab[-1]\n",
    "data_mab = data_mab[:-1]\n",
    "for i, elem in enumerate(data_mab):\n",
    "    elem[0] = f\"{elem[0]}_{i}\"\n",
    "    del elem[-1]\n",
    "    elem.append(times_online[i])\n",
    "headers = [\n",
    "        \"Model name\",\n",
    "        \"model pulls\",\n",
    "        \"Model Accuracy\",\n",
    "        \"Model loss\",\n",
    "        # \"Confidence interval\",\n",
    "        \"Runtime\"\n",
    "    ]\n",
    "\n",
    "print(tabulate(data, headers=headers, floatfmt=\".3f\", tablefmt=\"heavy_outline\"))\n",
    "# print(tabulate(data_mab, headers=headers, floatfmt=\".3f\", tablefmt=\"latex\"))\n",
    "print(runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# в одну табличку\n",
    "data_all = []\n",
    "for full_tr, online_tr in zip(data, data_mab):\n",
    "    elem = [full_tr[0], online_tr[1], online_tr[2], online_tr[4], full_tr[2], full_tr[4]]\n",
    "    data_all.append(elem)\n",
    "headers = [\n",
    "        \"Model name\",\n",
    "        \"model pulls\",\n",
    "        \"Online Accuracy\",\n",
    "        \"Online Runtime\",\n",
    "        \"Full Accuracy\",\n",
    "        \"Full Runtime\"\n",
    "    ]\n",
    "\n",
    "# print(tabulate(data, headers=headers, floatfmt=\".3f\", tablefmt=\"heavy_outline\"))\n",
    "print(tabulate(data_all, headers=headers, floatfmt=\".3f\", tablefmt=\"latex\"))\n",
    "print(runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_history(field_name):\n",
    "    b = [bb[2][field_name] for bb in bounds[:9]]\n",
    "    bounds_history = np.zeros((len(bounds), 9))\n",
    "    bounds_history[0] = b\n",
    "    for i, hist_el in enumerate(bounds[9:], 1):\n",
    "        arm = hist_el[0]\n",
    "        ucb = hist_el[2][field_name]\n",
    "        b[arm] = ucb\n",
    "        bounds_history[i] = b\n",
    "    bounds_history = bounds_history[:-9].T\n",
    "    return bounds_history\n",
    "\n",
    "loss_hist = get_history(\"loss\")\n",
    "bound_hist = get_history(\"ucb_val\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "colors = [\"r\", \"g\", \"black\", 'y', \"navy\", 'lightgreen', \"brown\", \"blue\", \"purple\"]\n",
    "for i, (loss, bound, color) in enumerate(zip(loss_hist, bound_hist, colors)):\n",
    "    plt.plot(loss, label = f\"ResNet-{i}\", color = color)\n",
    "    x = np.arange(len(loss))\n",
    "    plt.fill_between(x, bound, loss,color=color,\n",
    "                    alpha=0.1,)\n",
    "plt.ylim((-2, 3.5))\n",
    "plt.xlabel(r\"$\\#$ iterations.\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mlflow example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import os\n",
    "print(os.getcwd())\n",
    "if os.getcwd().endswith(\"/multiobjective_opt/notebooks\"):\n",
    "    os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = mlflow.get_experiment_by_name(\"cifar10_mix_80\")\n",
    "runs = mlflow.search_runs([runs.experiment_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "import json\n",
    "\n",
    "full_train_pos = runs[\"tags.mlflow.runName\"].apply(lambda x: x.startswith(\"full_train\"))\n",
    "run_id = runs[full_train_pos][\"run_id\"].values[0]\n",
    "artifact_path = mlflow.artifacts.download_artifacts(run_id=run_id, artifact_path=\"run_results_table.json\")\n",
    "\n",
    "with open(artifact_path, \"r\") as f:\n",
    "    f = json.load(f)\n",
    "\n",
    "print(tabulate(list(f.values())[1], headers=list(f.values())[0], floatfmt=\".3f\", tablefmt=\"heavy_outline\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "нарисуем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "exps = runs[runs['tags.mlflow.runName'].apply( lambda x: True)]\n",
    "for run_id in exps['run_id']:\n",
    "    try:\n",
    "        artifact_path = mlflow.artifacts.download_artifacts(run_id=run_id, artifact_path=\"run_results_table.json\")\n",
    "\n",
    "        with open(artifact_path, \"r\") as f:\n",
    "            tab = json.load(f)\n",
    "        break\n",
    "    except Exception as e:\n",
    "        continue\n",
    "    # print(tabulate(list(tab.values())[1], headers=list(tab.values())[0], floatfmt=\".3f\", tablefmt=\"heavy_outline\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[i[0] for i  in tab['data']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "durations = []\n",
    "for run_id in exps['run_id']:\n",
    "    artifact_path = mlflow.artifacts.download_artifacts(run_id=run_id, artifact_path=\"ucb_values.json\")\n",
    "    artifact_path\n",
    "    with open(artifact_path, \"r\") as f:\n",
    "            rf = json.load(f)\n",
    "    durations.append(rf[-1]['duration'])\n",
    "\n",
    "    # for i in range(7):\n",
    "    #     rf[i] = {\"arm\": rf[i][0],\"duration\": rf[i][1], \"pull_res\": rf[i][2]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "durations = np.array(durations)\n",
    "durations /= 60\n",
    "np.mean(durations), np.std(durations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_values(f, n_arms = 5):\n",
    "    def get_placeholder_for_val():\n",
    "        return np.zeros((len(f) - n_arms,n_arms))\n",
    "    duration = np.zeros(n_arms, float)\n",
    "    key_res = {key: get_placeholder_for_val() for key in f[0].keys()}\n",
    "    for i, elem in enumerate(f[:n_arms]):\n",
    "        # инициализируем значения\n",
    "        arm = elem['arm']\n",
    "        duration[arm] = elem[\"duration\"] - (f[i-1][\"duration\"] if i > 0 else 0)\n",
    "        for k, v in elem.items():\n",
    "            key_res[k][0][arm] = v\n",
    "    \n",
    "    # заполнение\n",
    "    for i, elem in enumerate(f[n_arms:-1], 1):\n",
    "        arm = elem['arm']\n",
    "        duration[arm] += elem[\"duration\"] - f[ i + n_arms - 2][\"duration\"]\n",
    "\n",
    "        for k, v in elem.items():\n",
    "            key_res[k][i] = key_res[k][i - 1]\n",
    "            key_res[k][i][arm] = v\n",
    "\n",
    "    return duration, key_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "client = mlflow.MlflowClient()\n",
    "keys = [\"arm\", \"loss\", \"accuracy\", \"test_accuracy\", \"test_loss\", \"duration\"]\n",
    "\n",
    "results = []\n",
    "for run_id in exps['run_id']:\n",
    "    # process for one run\n",
    "    # try:\n",
    "        run_res = [dict() for _ in range(355)]\n",
    "        for key in keys:\n",
    "                metric_h = client.get_metric_history(run_id, key=key)\n",
    "                if len(metric_h)<330:\n",
    "                      continue\n",
    "                for i, elem in enumerate(metric_h):\n",
    "                    run_res[i][key] = int(elem.value) if key == \"arm\" else elem.value\n",
    "            \n",
    "        print(run_res)\n",
    "        if len(run_res[0]) < 1:\n",
    "               continue\n",
    "        parsed_run = parse_values(run_res)\n",
    "        results.append(parsed_run)\n",
    "    # except IndexError as e:\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "run_loss_hist = []\n",
    "for run_id in exps['run_id']:\n",
    "    try:\n",
    "        artifact_path = mlflow.artifacts.download_artifacts(run_id=run_id, artifact_path=\"ucb_values.json\")\n",
    "        with open(artifact_path, \"r\") as f:\n",
    "\n",
    "            \n",
    "                rf = json.load(f)\n",
    "    except Exception as e:\n",
    "         continue\n",
    "    \n",
    "    if len(rf) >= 200:\n",
    "        duration, key_res = parse_values(rf)\n",
    "\n",
    "        loss_hist = key_res['loss'].T\n",
    "        run_loss_hist.append(loss_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "delta graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiobjective_opt.utils import LINESTYLES\n",
    "from multiobjective_opt.utils import get_fig_set_style\n",
    "LINESTYLES = [\n",
    "    (\"densely dashed\", (0, (3, 1))),\n",
    "    (\"d\", \"solid\"),\n",
    "    (\"d\", \"dotted\"),\n",
    "    (\"d\", \"dashdot\"),\n",
    "    (\"dashdotted\", (0, (3, 1, 3, ))),\n",
    "    (\"densely dashed\", (0, (5, 1))),\n",
    "    (\"densely dashdotdotted\", (0, (3, 1, 1, 1, 1, 1))),\n",
    "    (\"long dash with offset\", (1, (1, 0))),\n",
    "    (\"dashed\", (0, (5, 5))),\n",
    "    (\"d\", \"dashed\"),\n",
    "    (\"loosely dashdotdotted\", (0, (3, 10, 1, 10, 1, 10))),\n",
    "    (\"densely dashdotted\", (0, (3, 1, 1, 1))),\n",
    "    (\"dashdotdotted\", (0, (3, 5, 1, 5, 1, 5))),\n",
    "    (\"densely dotted\", (0, (1, 1))),\n",
    "    (\"long dash with offset\", (5, (10, 3))),\n",
    "    (\"loosely dashdotted\", (0, (3, 10, 1, 10))),\n",
    "    (\"densely dashdotdotted\", (0, (3, 1, 1, 1, 1, 1))),\n",
    "    (\"loosely dashed\", (0, (5, 10))),\n",
    "    (\"dashdotdotted\", (0, (3, 5, 1, 5, 1, 5))),\n",
    "    (\"loosely dashdotdotted\", (0, (3, 10, 1, 10, 1, 10))),\n",
    "]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "# def get_loss_hist(key = \"loss\"):\n",
    "#     run_loss_hist = []\n",
    "\n",
    "#     for run_id in exps['run_id']:\n",
    "#         try:\n",
    "#             artifact_path = mlflow.artifacts.download_artifacts(run_id=run_id, artifact_path=\"ucb_values.json\")\n",
    "#             with open(artifact_path, \"r\") as f:\n",
    "#                     rf = json.load(f)\n",
    "#         except Exception as e:\n",
    "#              continue\n",
    "#         # print(rf)\n",
    "#         # if len(rf) == 200:\n",
    "#         duration, key_res = parse_values(rf)\n",
    "\n",
    "#         loss_hist = key_res[key].T\n",
    "#         run_loss_hist.append(loss_hist)\n",
    "#     return run_loss_hist\n",
    "\n",
    "def get_loss_hist(key = \"loss\"):\n",
    "    run_loss_hist = [r[1][key].T for r in results]\n",
    "    return run_loss_hist\n",
    "\n",
    "\n",
    "\n",
    "# from multiobjective_opt.utils import LINESTYLES, get_fig_set_style\n",
    "def plot_vals(ax, run_hist, ylabel, ylim):\n",
    "    losses = np.stack(run_hist)\n",
    "    mean_loss = losses.mean(0)\n",
    "    low_loss = np.quantile(losses, 0.1, 0)\n",
    "    high_loss = np.quantile(losses, 0.9, 0)\n",
    "\n",
    "\n",
    "    colors = [\"r\", \"g\", \"black\", \"navy\", \"brown\", \"blue\", \"purple\", 'darkgreen',]\n",
    "    markers = [\"<\", \"o\", \"D\", \">\", 's']\n",
    "    nums = [19, 18, 25, 20, 22]\n",
    "    # model_names = [elem[0] for elem in list(tab.values())[1]]\n",
    "    model_names = [\"VGG\", \"Resnet18\", \"ShallowMLP\", \"DeepMLP\", \"DeepMLPNorm\"]\n",
    "    model_names = ['ShallowMLP', 'ResNet18', 'DeepMLPNorm', 'VGGLike', 'DeepMLP']\n",
    "    # model_names = [\"Linear\", \"MLP\", \"Conv2Layer\", \"Conv3Layer\", \"ConvDropout\", \"ConvBatchNorm\", \"ResNet18\"]\n",
    "    for i, (loss, low, high, color, m_name, ls, m, n) in enumerate(zip(mean_loss, low_loss, high_loss, colors, model_names, LINESTYLES, markers, nums)):\n",
    "        # plt.fill_between(x, loss - std,loss + std , color=color, alpha=0.1,)\n",
    "        if m_name == \"VGGLike\":\n",
    "             m_name = \"VGG\"\n",
    "        if \"Conv\" in m_name:\n",
    "             continue\n",
    "        ax.plot(loss, label = m_name, color = color, linestyle=ls[1], marker = m, \n",
    "                            markersize = 10, \n",
    "                            markeredgewidth=2, \n",
    "                            markerfacecolor='white',\n",
    "                            markeredgecolor='black',\n",
    "                            markevery = n)\n",
    "        print(m_name, ls[0])\n",
    "        x = np.arange(len(loss))\n",
    "        # if m_name in [\"ResNet18\", \"ConvBatchNorm\"]:\n",
    "        ax.fill_between(x, low, high, color=color, alpha=0.1,)\n",
    "\n",
    "    ax.set_ylim(ylim)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_xlabel(r\"$\\#$ iterations\")\n",
    "    ax.grid()\n",
    "    # ax.legend()\n",
    "\n",
    "fig, ax, _ = get_fig_set_style(7, shape=(1,2), figsize=(12,6))\n",
    "ax[1].grid()\n",
    "plot_vals(ax[0], get_loss_hist(\"accuracy\"), \"Validation accuracy@1\", (0.08, 0.85))\n",
    "plot_vals(ax[1], get_loss_hist(\"loss\"), \" Validation loss\", (0.5, 2.5))\n",
    "h, legend_ = ax[0].get_legend_handles_labels()\n",
    "\n",
    "pos = [1, 0, 3, 4, 2]\n",
    "print(legend_)\n",
    "legend_, h = [legend_[p] for p in pos], [h[p] for p in pos]\n",
    "fig.legend(\n",
    "    h,\n",
    "    legend_,\n",
    "    ncol=3,\n",
    "    bbox_to_anchor=(0.0, -0.06, 1, 0.10),\n",
    "    loc=\"outside upper left\",\n",
    "    mode=\"expand\",\n",
    "    borderaxespad=0,\n",
    ")\n",
    "\n",
    "fig.tight_layout()\n",
    "# fig = fig.subplots_adjust(bottom=0.1)\n",
    "figs = {\"both\": fig}\n",
    "\n",
    "fig, ax, _ = get_fig_set_style(7, shape=(1,2), figsize=(12,6))\n",
    "ax[1].grid()\n",
    "plot_vals(ax[0], get_loss_hist(\"test_accuracy\"), \"Test accuracy@1\", (0.08, 0.85))\n",
    "plot_vals(ax[1], get_loss_hist(\"test_loss\"), \" Test loss\", (0.5, 2.5))\n",
    "h, legend_ = ax[0].get_legend_handles_labels()\n",
    "\n",
    "pos = [1, 0, 3, 4, 2]\n",
    "print(legend_)\n",
    "legend_, h = [legend_[p] for p in pos], [h[p] for p in pos]\n",
    "fig.legend(\n",
    "    h,\n",
    "    legend_,\n",
    "    ncol=3,\n",
    "    bbox_to_anchor=(0.0, -0.06, 1, 0.10),\n",
    "    loc=\"outside upper left\",\n",
    "    mode=\"expand\",\n",
    "    borderaxespad=0,\n",
    ")\n",
    "\n",
    "fig.tight_layout()\n",
    "# fig = fig.subplots_adjust(bottom=0.1)\n",
    "figs = {\"test_both\": fig}\n",
    "# ax.grid()\n",
    "\n",
    "\n",
    "# figs = {}\n",
    "# figs[\"acc\"] = plot_vals(get_loss_hist(\"accuracy\"), \"Accuracy\", (0.2, 1.))\n",
    "# figs[\"loss\"] = plot_vals(get_loss_hist(\"loss\"), \"Loss\", (0.5, 2.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from multiobjective_opt.utils import savefig\n",
    "savepath = Path(\"./exp_results/figures/cv\")\n",
    "# exp_results/figures\n",
    "for f_name, fig in figs.items():\n",
    "    path = savepath\n",
    "    if not path.exists():\n",
    "        os.mkdir(path)\n",
    "    f_name = f\"{f_name}\"\n",
    "    savefig(fig, path, f_name, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### тренировка до точности в 1 процент"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "if os.getcwd().endswith(\"notebooks\"):\n",
    "    os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itsdangerous import TimestampSigner\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "# Инициализация клиента MLflow\n",
    "client = MlflowClient()\n",
    "\n",
    "# Название родительского эксперимента и запуска\n",
    "experiment_name = \"cifar10_simple_min_log_fixed\"\n",
    "parent_run_name = \"full_train:08:08\"\n",
    "# parent_run_name = \"full_train:10:51\"\n",
    "\n",
    "# Шаг 1: Найти родительский запуск\n",
    "experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "if experiment is None:\n",
    "    raise ValueError(f\"Эксперимент '{experiment_name}' не найден.\")\n",
    "\n",
    "# Поиск родительского запуска по имени\n",
    "parent_runs = client.search_runs(\n",
    "    experiment_ids=[experiment.experiment_id],\n",
    "    filter_string=f\"tags.mlflow.runName = '{parent_run_name}'\"\n",
    ")\n",
    "\n",
    "if not parent_runs:\n",
    "    raise ValueError(f\"Родительский запуск '{parent_run_name}' не найден.\")\n",
    "\n",
    "parent_run = parent_runs[0]\n",
    "parent_run_id = parent_run.info.run_id\n",
    "\n",
    "# Шаг 2: Найти все дочерние запуски\n",
    "child_runs = client.search_runs(\n",
    "    experiment_ids=[experiment.experiment_id],\n",
    "    filter_string=f\"tags.mlflow.parentRunId = '{parent_run_id}'\"\n",
    ")\n",
    "\n",
    "# Шаг 3: Обработка результатов дочерних запусков\n",
    "\n",
    "def get_loss(key):\n",
    "    losses = []\n",
    "    timestamps = []\n",
    "    alg_names = []\n",
    "    for run in child_runs:\n",
    "        run_id = run.info.run_id\n",
    "        run_name = run.data.tags.get(\"mlflow.runName\", \"Unnamed Run\")\n",
    "        values = client.get_metric_history(run_id=run.info.run_id, key=key)\n",
    "        loss = [l.value for l in values]\n",
    "        times = [l.timestamp for l in values]\n",
    "        \n",
    "        losses.append(loss)\n",
    "        timestamps.append(times)\n",
    "        alg_names.append(run_name)\n",
    "        # losses[run_name] = loss\n",
    "        # timestamps[run_name] = times\n",
    "    return losses, timestamps, alg_names\n",
    "\n",
    "losses, timestamps, alg_names = get_loss(\"accuracy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_start = 0\n",
    "times = []\n",
    "fig, ax = plt.subplots()\n",
    "for i, (l, t, n) in enumerate(zip(losses, timestamps, alg_names)):\n",
    "    x = np.array(t) -t[0] + t_start\n",
    "    t_start = x[-1]\n",
    "    x = x/1000\n",
    "    times.append(x)\n",
    "    ax.plot(x, l,label = n)\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = np.concat(times)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itsdangerous import TimestampSigner\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "\n",
    "def get_duration(timestamps):\n",
    "    t_start = 0\n",
    "    t_end = 0\n",
    "    for t in timestamps:\n",
    "        x = np.array(t) -t[0] + t_start\n",
    "        t_start = x[-1]\n",
    "        x = x/1000\n",
    "        t_end = x[-1]\n",
    "    return t_end\n",
    "\n",
    "def get_loss(key, child_runs):\n",
    "    # losses = []\n",
    "    timestamps = []\n",
    "    # alg_names = []\n",
    "    for run in child_runs:\n",
    "        run_id = run.info.run_id\n",
    "        run_name = run.data.tags.get(\"mlflow.runName\", \"Unnamed Run\")\n",
    "        values = client.get_metric_history(run_id=run.info.run_id, key=key)\n",
    "        loss = [l.value for l in values]\n",
    "        times = [l.timestamp for l in values]\n",
    "        \n",
    "        losses.append(loss)\n",
    "        timestamps.append(times)\n",
    "        alg_names.append(run_name)\n",
    "        # losses[run_name] = loss\n",
    "        # timestamps[run_name] = times\n",
    "    return get_duration(timestamps)\n",
    "\n",
    "\n",
    "# Инициализация клиента MLflow\n",
    "client = MlflowClient()\n",
    "\n",
    "# Название родительского эксперимента и запуска\n",
    "experiment_name = \"cifar10_simple_min_log_fixed\"\n",
    "parent_run_name_st = \"full_train\"\n",
    "\n",
    "# Шаг 1: Найти родительский запуск\n",
    "experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "if experiment is None:\n",
    "    raise ValueError(f\"Эксперимент '{experiment_name}' не найден.\")\n",
    "\n",
    "# Поиск родительского запуска по имени\n",
    "parent_runs = client.search_runs(\n",
    "    experiment_ids=[experiment.experiment_id],\n",
    "    filter_string=f\"tags.mlflow.runName LIKE '{parent_run_name_st}%'\"\n",
    ")\n",
    "\n",
    "if not parent_runs:\n",
    "    raise ValueError(f\"Родительский запуск '{parent_run_name}' не найден.\")\n",
    "\n",
    "runtimes = []\n",
    "for parent_run in parent_runs:\n",
    "    parent_run_id = parent_run.info.run_id\n",
    "\n",
    "    # Шаг 2: Найти все дочерние запуски\n",
    "    child_runs = client.search_runs(\n",
    "        experiment_ids=[experiment.experiment_id],\n",
    "        filter_string=f\"tags.mlflow.parentRunId = '{parent_run_id}'\"\n",
    "    )\n",
    "\n",
    "\n",
    "    runtimes.append(get_loss(\"accuracy\", child_runs)/60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(runtimes), np.std(runtimes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiobjective_opt.neural_net.utils.dataset_prepare import CIFAR10Handler\n",
    "handler = CIFAR10Handler()\n",
    "tr, test = handler.load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for _ in tr.get_iterator():\n",
    "    i += 1\n",
    "i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## табличка с параметрами моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from multiobjective_opt.neural_net.models.cifar_models import (\n",
    "    CNN,\n",
    "    MLP,\n",
    "    CNNBatchNorm,\n",
    "    CNNDropout,\n",
    "    DeepCNN,\n",
    "    ResNet18,\n",
    "    SimpleLinearModel,\n",
    ")\n",
    "model_names_new = [\"Linear\", \"MLP\", \"Conv2Layer\", \"Conv3Layer\", \"ConvDropout\", \"ConvBatchNorm\", \"ResNet18\"]\n",
    "\n",
    "from multiobjective_opt.neural_net.models.cifar_models import (\n",
    "    CNN,\n",
    "    MLP,\n",
    "    CNNBatchNorm,\n",
    "    CNNDropout,\n",
    "    DeepCNN,\n",
    "    ResNet18,\n",
    "    SimpleLinearModel,\n",
    ")\n",
    "from multiobjective_opt.neural_net.models.cifar_models import (\n",
    "    ResNet18Torch,\n",
    "    DeepMLP,\n",
    "    ShallowMLP,\n",
    "    VGGLike,\n",
    "    ViTModel\n",
    ")\n",
    "from multiobjective_opt.neural_net.models.cifar_vit import ViTSmall\n",
    "\n",
    "def get_models():\n",
    "    models = {\n",
    "        # \"SimpleLinearModel\": SimpleLinearModel(),\n",
    "        # \"FullyConnectedModel\": MLP(),\n",
    "        # \"Conv2LayerModel\": CNN(),\n",
    "        # \"Conv3LayerModel\": DeepCNN(),\n",
    "        \"ShallowMLP\": ShallowMLP(),\n",
    "        \"DeepMLP\": DeepMLP(False),\n",
    "        \"DeepMLPNorm\": DeepMLP(),\n",
    "        \"VGGLike\": VGGLike(),\n",
    "        \"ResNet18\": ResNet18(),\n",
    "        # \"ConvDropout\": CNNDropout(),\n",
    "        # \"ConvBatchNorm\": CNNBatchNorm(),\n",
    "        # \"ResNet18_no\": ResNet18(with_shortcut=False),\n",
    "        # \"ResNet18Torch\": ResNet18Torch(),\n",
    "            # \"ResNet18\": ResNet18Torch(),\n",
    "            # \"ViTModel\": ViTModel()\n",
    "            # \"ViTSmall\": ViTSmall()\n",
    "    }\n",
    "    return models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# from torchsummary import summary\n",
    "from tabulate import tabulate\n",
    "\n",
    "def analyze_model(model):\n",
    "    # Словарь для хранения информации о модели\n",
    "    model_info = {\n",
    "        \"Total Parameters\": 0,\n",
    "        \"Conv Layers\": 0,\n",
    "        \"Conv Parameters\": 0,\n",
    "        \"Normalization Layers\": 0,\n",
    "        # \"Dropout Layers\": 0,\n",
    "    }\n",
    "\n",
    "    # Перебор всех модулей модели\n",
    "    for name, module in model.named_modules():\n",
    "        # Подсчет общего числа параметров\n",
    "        if isinstance(module, nn.Conv2d):\n",
    "            model_info[\"Conv Layers\"] += 1\n",
    "            model_info[\"Conv Parameters\"] += sum(p.numel() for p in module.parameters())\n",
    "        elif isinstance(module, (nn.BatchNorm2d,nn.BatchNorm1d, nn.LayerNorm, nn.GroupNorm)):\n",
    "            model_info[\"Normalization Layers\"] += 1\n",
    "        # elif isinstance(module, nn.Dropout):\n",
    "            # model_info[\"Dropout Layers\"] += 1\n",
    "\n",
    "    # Подсчет общего числа параметров\n",
    "    model_info[\"Total Parameters\"] = sum(p.numel() for p in model.parameters())\n",
    "    model_info[\"Total Parameters\"] = f\"{model_info[\"Total Parameters\"]/1e6:.2f}M\"\n",
    "    model_info[\"Conv Parameters\"] = f\"{model_info[\"Conv Parameters\"]/1e6:.2f}M\"\n",
    "    return model_info\n",
    "\n",
    "def print_model_table(models):\n",
    "    # Создание таблицы\n",
    "    table = []\n",
    "    headers = [\"Model\", \"Total Parameters\", \"Conv Layers\", \"Conv Parameters\", r\"$\\#$ Normalizatios\", r\"$\\#$ Dropouts\"]\n",
    "\n",
    "    for name, model in models.items():\n",
    "        info = analyze_model(model)\n",
    "        table.append([name, info[\"Total Parameters\"], info[\"Conv Layers\"], info[\"Conv Parameters\"], info[\"Normalization Layers\"], ])\n",
    "\n",
    "    # Вывод таблицы\n",
    "    print(tabulate(table, headers=headers, tablefmt=\"latex_booktabs\"))\n",
    "\n",
    "# Пример использования\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc = nn.Linear(64 * 28 * 28, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# # Создание моделей\n",
    "# model1 = SimpleCNN()\n",
    "# model2 = SimpleCNN()  # Вторая модель для примера\n",
    "\n",
    "# Анализ и вывод таблицы\n",
    "# models = {\"Model 1\": model1, \"Model 2\": model2}\n",
    "print_model_table(get_models(), )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.rand(5,3,32,32)\n",
    "ResNet18(with_shortcut=False)(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from multiobjective_opt.neural_net.models.cifar_models import (\n",
    "    ResNet18Torch,\n",
    "    DeepMLP,\n",
    "    ShallowMLP,\n",
    "    VGGLike,\n",
    "    ViTModel\n",
    ")\n",
    "\n",
    "def get_models():\n",
    "    models = {\n",
    "        \"ResNet18\": ResNet18Torch(),\n",
    "        \"DeepMLP_norm\": DeepMLP(),\n",
    "        \"DeepMLP\": DeepMLP(False),\n",
    "        \"ShallowMLP\": ShallowMLP(),\n",
    "        \"VGGLike\": VGGLike(),\n",
    "        \"ViTModel\": ViTModel()\n",
    "    }\n",
    "\n",
    "    return models\n",
    "models = get_models()\n",
    "\n",
    "print_model_table(models, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/lucidrains/vit-pytorch/blob/main/vit_pytorch/vit.py\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "# helpers\n",
    "\n",
    "def pair(t):\n",
    "    return t if isinstance(t, tuple) else (t, t)\n",
    "\n",
    "# classes\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head *  heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.attend = nn.Softmax(dim = -1)\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n",
    "\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "\n",
    "        attn = self.attend(dots)\n",
    "\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)),\n",
    "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout))\n",
    "            ]))\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "        return x\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, pool = 'cls', channels = 3, dim_head = 64, dropout = 0., emb_dropout = 0.):\n",
    "        super().__init__()\n",
    "        image_height, image_width = pair(image_size)\n",
    "        patch_height, patch_width = pair(patch_size)\n",
    "\n",
    "        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
    "\n",
    "        num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
    "        patch_dim = channels * patch_height * patch_width\n",
    "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
    "\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width),\n",
    "            nn.Linear(patch_dim, dim),\n",
    "        )\n",
    "\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
    "\n",
    "        self.pool = pool\n",
    "        self.to_latent = nn.Identity()\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        x = self.to_patch_embedding(img)\n",
    "        b, n, _ = x.shape\n",
    "\n",
    "        cls_tokens = repeat(self.cls_token, '() n d -> b n d', b = b)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x += self.pos_embedding[:, :(n + 1)]\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n",
    "\n",
    "        x = self.to_latent(x)\n",
    "        return self.mlp_head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/lucidrains/vit-pytorch/blob/main/vit_pytorch/vit_for_small_dataset.py\n",
    "\n",
    "from math import sqrt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "# helpers\n",
    "\n",
    "def pair(t):\n",
    "    return t if isinstance(t, tuple) else (t, t)\n",
    "\n",
    "# classes\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class LSA(nn.Module):\n",
    "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head *  heads\n",
    "        self.heads = heads\n",
    "        self.temperature = nn.Parameter(torch.log(torch.tensor(dim_head ** -0.5)))\n",
    "\n",
    "        self.attend = nn.Softmax(dim = -1)\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n",
    "\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.temperature.exp()\n",
    "\n",
    "        mask = torch.eye(dots.shape[-1], device = dots.device, dtype = torch.bool)\n",
    "        mask_value = -torch.finfo(dots.dtype).max\n",
    "        dots = dots.masked_fill(mask, mask_value)\n",
    "\n",
    "        attn = self.attend(dots)\n",
    "\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, LSA(dim, heads = heads, dim_head = dim_head, dropout = dropout)),\n",
    "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout))\n",
    "            ]))\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "        return x\n",
    "\n",
    "class SPT(nn.Module):\n",
    "    def __init__(self, *, dim, patch_size, channels = 3):\n",
    "        super().__init__()\n",
    "        patch_dim = patch_size * patch_size * 5 * channels\n",
    "\n",
    "        self.to_patch_tokens = nn.Sequential(\n",
    "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_size, p2 = patch_size),\n",
    "            nn.LayerNorm(patch_dim),\n",
    "            nn.Linear(patch_dim, dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        shifts = ((1, -1, 0, 0), (-1, 1, 0, 0), (0, 0, 1, -1), (0, 0, -1, 1))\n",
    "        shifted_x = list(map(lambda shift: F.pad(x, shift), shifts))\n",
    "        x_with_shifts = torch.cat((x, *shifted_x), dim = 1)\n",
    "        return self.to_patch_tokens(x_with_shifts)\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, pool = 'cls', channels = 3, dim_head = 64, dropout = 0., emb_dropout = 0.):\n",
    "        super().__init__()\n",
    "        image_height, image_width = pair(image_size)\n",
    "        patch_height, patch_width = pair(patch_size)\n",
    "\n",
    "        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
    "\n",
    "        num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
    "        patch_dim = channels * patch_height * patch_width\n",
    "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
    "\n",
    "        self.to_patch_embedding = SPT(dim = dim, patch_size = patch_size, channels = channels)\n",
    "\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
    "\n",
    "        self.pool = pool\n",
    "        self.to_latent = nn.Identity()\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        x = self.to_patch_embedding(img)\n",
    "        b, n, _ = x.shape\n",
    "\n",
    "        cls_tokens = repeat(self.cls_token, '() n d -> b n d', b = b)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x += self.pos_embedding[:, :(n + 1)]\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n",
    "\n",
    "        x = self.to_latent(x)\n",
    "        return self.mlp_head(x)\n",
    "    \n",
    "\n",
    "net = ViT(\n",
    "image_size = (32,32),\n",
    "patch_size = 4,\n",
    "num_classes = 10,\n",
    "dim = int(512),\n",
    "depth = 4,\n",
    "heads = 6,\n",
    "mlp_dim = 256,\n",
    "dropout = 0.1,\n",
    "emb_dropout = 0.1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "net = ViT(\n",
    "image_size = (32,32),\n",
    "patch_size = 4,\n",
    "num_classes = 10,\n",
    "dim = int(512),\n",
    "depth = 4,\n",
    "heads = 6,\n",
    "mlp_dim = 256,\n",
    "dropout = 0.1,\n",
    "emb_dropout = 0.1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.rand((5, 3,32,32))\n",
    "\n",
    "net(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_model_table({\"TVIT\": net})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
